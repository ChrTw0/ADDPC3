"""
An√°lisis de Validaci√≥n de Metodolog√≠a - Conectando a MySQL
===========================================================
Este script se conecta directamente a la base de datos MySQL 'denuncias_peru'
para validar si vale la pena hacer predicciones de hotspots.

Preguntas cr√≠ticas:
1. ¬øLos datos originales tienen suficiente calidad?
2. ¬øExisten patrones temporales predecibles?
3. ¬øExisten hotspots espaciales persistentes?
4. ¬øLa metodolog√≠a es correcta?
5. ¬øUn R¬≤ de 0.697 es realmente bueno?
6. ¬øVALE LA PENA hacer predicciones o es ruido aleatorio?
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sqlalchemy import create_engine, text
from scipy import stats
from sklearn.metrics import r2_score
from dotenv import load_dotenv
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n visual
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("="*80)
print("VALIDACI√ìN DE METODOLOG√çA - AN√ÅLISIS DESDE DATOS ORIGINALES (MySQL)")
print("="*80)
print()

# ============================================================================
# 1. CONEXI√ìN A BASE DE DATOS
# ============================================================================
print("1. CONECTANDO A BASE DE DATOS MySQL...")
print("-" * 80)

load_dotenv()
db_user = os.getenv("MYSQL_USER")
db_password = os.getenv("MYSQL_PASSWORD")
db_host = os.getenv("MYSQL_HOST")
db_name = os.getenv("MYSQL_DB")

print(f"Host: {db_host}")
print(f"Base de datos: {db_name}")
print(f"Usuario: {db_user}")

try:
    connection_string = f"mysql+mysqlconnector://{db_user}:{db_password}@{db_host}/{db_name}"
    engine = create_engine(connection_string)
    connection = engine.connect()
    print("‚úì Conexi√≥n exitosa a MySQL")
    connection.close()
except Exception as e:
    print(f"‚ùå Error de conexi√≥n: {e}")
    sys.exit(1)

# ============================================================================
# 2. EXPLORACI√ìN DE LA ESTRUCTURA DE DATOS
# ============================================================================
print("\n" + "="*80)
print("2. EXPLORANDO ESTRUCTURA DE LA TABLA 'denuncias'")
print("="*80)

# Obtener info b√°sica
query_count = text("SELECT COUNT(*) as total FROM denuncias")
total_records = pd.read_sql(query_count, engine).iloc[0]['total']
print(f"\n‚úì Total de registros en la tabla: {total_records:,}")

# Columnas disponibles
query_columns = text("SHOW COLUMNS FROM denuncias")
columns_info = pd.read_sql(query_columns, engine)
print(f"\n‚úì Columnas disponibles ({len(columns_info)}):")
for idx, row in columns_info.iterrows():
    print(f"   - {row['Field']}: {row['Type']}")

# Modalidades de hecho disponibles
query_modalidades = text("""
    SELECT modalidad_hecho, COUNT(*) as count
    FROM denuncias
    GROUP BY modalidad_hecho
    ORDER BY count DESC
    LIMIT 10
""")
modalidades = pd.read_sql(query_modalidades, engine)
print(f"\n‚úì Top 10 Modalidades de Hecho:")
for idx, row in modalidades.iterrows():
    print(f"   {idx+1}. {row['modalidad_hecho']}: {row['count']:,} denuncias")

# Departamentos disponibles
query_deptos = text("""
    SELECT departamento_hecho, COUNT(*) as count
    FROM denuncias
    GROUP BY departamento_hecho
    ORDER BY count DESC
    LIMIT 10
""")
departamentos = pd.read_sql(query_deptos, engine)
print(f"\n‚úì Top 10 Departamentos:")
for idx, row in departamentos.iterrows():
    print(f"   {idx+1}. {row['departamento_hecho']}: {row['count']:,} denuncias")

# ============================================================================
# 3. EXTRACCI√ìN DE DATOS: ROBO AGRAVADO EN LIMA
# ============================================================================
print("\n" + "="*80)
print("3. EXTRAYENDO DATOS: ROBO AGRAVADO EN LIMA")
print("="*80)

query_lima = text("""
    SELECT
        id,
        lat_hecho,
        long_hecho,
        fecha_hora_hecho,
        modalidad_hecho,
        departamento_hecho,
        provincia_hecho,
        distrito_hecho
    FROM denuncias
    WHERE departamento_hecho = 'LIMA'
    AND modalidad_hecho = 'ROBO AGRAVADO'
""")

print("Cargando datos desde MySQL... (esto puede tomar un momento)")
df = pd.read_sql(query_lima, engine)
print(f"‚úì Datos cargados: {len(df):,} registros de Robo Agravado en Lima")

# ============================================================================
# 4. LIMPIEZA Y VALIDACI√ìN DE DATOS
# ============================================================================
print("\n" + "="*80)
print("4. LIMPIEZA Y VALIDACI√ìN DE CALIDAD DE DATOS")
print("="*80)

print("\n4.1 Valores Nulos por Columna:")
nulls = df.isnull().sum()
for col, count in nulls.items():
    if count > 0:
        pct = count / len(df) * 100
        print(f"   {col}: {count:,} ({pct:.2f}%)")

# Limpiar coordenadas
df['lat_hecho'] = pd.to_numeric(df['lat_hecho'], errors='coerce')
df['long_hecho'] = pd.to_numeric(df['long_hecho'], errors='coerce')
df['fecha_hora_hecho'] = pd.to_datetime(df['fecha_hora_hecho'], errors='coerce')

inicial = len(df)
df = df.dropna(subset=['lat_hecho', 'long_hecho', 'fecha_hora_hecho'])
final = len(df)
removidos = inicial - final

print(f"\n4.2 Limpieza de Datos:")
print(f"   Registros iniciales: {inicial:,}")
print(f"   Registros removidos: {removidos:,} ({removidos/inicial*100:.2f}%)")
print(f"   Registros v√°lidos: {final:,}")

# Validar rango geogr√°fico de Lima
lat_lima = (-12.3, -11.7)
long_lima = (-77.2, -76.7)

coords_validas = (
    (df['lat_hecho'] >= lat_lima[0]) &
    (df['lat_hecho'] <= lat_lima[1]) &
    (df['long_hecho'] >= long_lima[0]) &
    (df['long_hecho'] <= long_lima[1])
)

print(f"\n4.3 Validaci√≥n de Coordenadas:")
print(f"   Dentro del rango de Lima: {coords_validas.sum():,} / {len(df):,} ({coords_validas.sum()/len(df)*100:.2f}%)")
print(f"   Fuera del rango: {(~coords_validas).sum():,}")

if (~coords_validas).sum() > 0:
    print(f"\n   ‚ö†Ô∏è  Hay {(~coords_validas).sum()} coordenadas fuera del rango esperado de Lima")
    print(f"   Rango latitud: [{df['lat_hecho'].min():.4f}, {df['lat_hecho'].max():.4f}]")
    print(f"   Rango longitud: [{df['long_hecho'].min():.4f}, {df['long_hecho'].max():.4f}]")

# Rango temporal
print(f"\n4.4 Rango Temporal:")
fecha_min = df['fecha_hora_hecho'].min()
fecha_max = df['fecha_hora_hecho'].max()
dias_total = (fecha_max - fecha_min).days
print(f"   Desde: {fecha_min}")
print(f"   Hasta: {fecha_max}")
print(f"   Per√≠odo: {dias_total} d√≠as (~{dias_total/365:.1f} a√±os)")

# ============================================================================
# 5. AN√ÅLISIS TEMPORAL - ¬øHAY PATRONES PREDECIBLES?
# ============================================================================
print("\n" + "="*80)
print("5. AN√ÅLISIS TEMPORAL - ¬øEXISTEN PATRONES PREDECIBLES?")
print("="*80)

df['a√±o'] = df['fecha_hora_hecho'].dt.year
df['mes'] = df['fecha_hora_hecho'].dt.month
df['dia_semana'] = df['fecha_hora_hecho'].dt.dayofweek
df['a√±o_semana'] = df['fecha_hora_hecho'].dt.strftime('%Y-%U')

print("\n5.1 Distribuci√≥n por A√±o:")
por_a√±o = df.groupby('a√±o').size()
for a√±o, count in por_a√±o.items():
    pct = count / len(df) * 100
    print(f"   {a√±o}: {count:,} ({pct:.1f}%)")

print("\n5.2 Distribuci√≥n por Mes:")
por_mes = df.groupby('mes').size().sort_index()
meses = ['Ene', 'Feb', 'Mar', 'Abr', 'May', 'Jun', 'Jul', 'Ago', 'Sep', 'Oct', 'Nov', 'Dic']
for mes, count in por_mes.items():
    pct = count / len(df) * 100
    print(f"   {meses[mes-1]}: {count:,} ({pct:.1f}%)")

cv_mes = por_mes.std() / por_mes.mean()
print(f"\n   Coeficiente de Variaci√≥n (CV): {cv_mes:.4f}")
if cv_mes > 0.15:
    print(f"   ‚úì ALTA variabilidad mensual - HAY ESTACIONALIDAD")
else:
    print(f"   ‚ö†Ô∏è  Baja variabilidad - Patr√≥n mensual d√©bil")

print("\n5.3 Distribuci√≥n por D√≠a de la Semana:")
por_dia = df.groupby('dia_semana').size().sort_index()
dias_nom = ['Lunes', 'Martes', 'Mi√©rcoles', 'Jueves', 'Viernes', 'S√°bado', 'Domingo']
for dia, count in por_dia.items():
    pct = count / len(df) * 100
    print(f"   {dias_nom[dia]}: {count:,} ({pct:.1f}%)")

cv_dia = por_dia.std() / por_dia.mean()
print(f"\n   Coeficiente de Variaci√≥n (CV): {cv_dia:.4f}")
if cv_dia > 0.10:
    print(f"   ‚úì HAY patr√≥n semanal predecible")
else:
    print(f"   ‚ö†Ô∏è  Patr√≥n semanal d√©bil")

# Autocorrelaci√≥n temporal
print("\n5.4 Autocorrelaci√≥n Temporal (cr√≠menes por semana):")
crimes_per_week = df.groupby('a√±o_semana').size().sort_index()
print(f"   Total de semanas: {len(crimes_per_week)}")
print(f"   Promedio cr√≠menes/semana: {crimes_per_week.mean():.2f}")
print(f"   Desv. est√°ndar: {crimes_per_week.std():.2f}")

lags = [1, 2, 4, 8, 12]
print("\n   Correlaci√≥n con semanas anteriores:")
for lag in lags:
    if len(crimes_per_week) > lag:
        shifted = crimes_per_week.shift(lag)
        valid_mask = ~(crimes_per_week.isna() | shifted.isna())
        if valid_mask.sum() > 10:
            corr = crimes_per_week[valid_mask].corr(shifted[valid_mask])
            status = '‚úì Fuerte' if abs(corr) > 0.5 else '‚úì Moderada' if abs(corr) > 0.3 else '‚ö†Ô∏è  D√©bil'
            print(f"   Lag {lag:2d} semana(s): {corr:6.4f} {status}")

# ============================================================================
# 6. AN√ÅLISIS ESPACIAL - ¬øHAY HOTSPOTS PERSISTENTES?
# ============================================================================
print("\n" + "="*80)
print("6. AN√ÅLISIS ESPACIAL - ¬øEXISTEN HOTSPOTS PERSISTENTES?")
print("="*80)

# Crear grid (0.005 grados ‚âà 555 metros en Lima)
grid_size = 0.005
df['grid_lat'] = (df['lat_hecho'] // grid_size) * grid_size
df['grid_long'] = (df['long_hecho'] // grid_size) * grid_size
df['grid_cell'] = df['grid_lat'].astype(str) + '_' + df['grid_long'].astype(str)

crimes_per_cell = df.groupby('grid_cell').size().sort_values(ascending=False)

print(f"\n6.1 Estad√≠sticas Espaciales:")
print(f"   Total de celdas: {len(crimes_per_cell):,}")
print(f"   Promedio cr√≠menes/celda: {crimes_per_cell.mean():.2f}")
print(f"   Mediana cr√≠menes/celda: {crimes_per_cell.median():.2f}")
print(f"   Desv. est√°ndar: {crimes_per_cell.std():.2f}")
print(f"   Coeficiente de Variaci√≥n: {crimes_per_cell.std() / crimes_per_cell.mean():.4f}")

print(f"\n6.2 Top 10 Hotspots:")
for i, (cell, count) in enumerate(crimes_per_cell.head(10).items(), 1):
    pct = count / len(df) * 100
    print(f"   {i:2d}. Celda {cell}: {count:,} cr√≠menes ({pct:.2f}% del total)")

# Concentraci√≥n tipo Pareto
print(f"\n6.3 Concentraci√≥n Espacial (Principio de Pareto):")
percentiles = [5, 10, 20, 30]
for pct in percentiles:
    top_n = int(len(crimes_per_cell) * pct / 100)
    top_crimes = crimes_per_cell.head(top_n).sum()
    print(f"   Top {pct:2d}% de celdas ‚Üí {top_crimes:,} cr√≠menes ({top_crimes/len(df)*100:.1f}% del total)")

# √çndice de Gini
sorted_crimes = np.sort(crimes_per_cell.values)
n = len(sorted_crimes)
index = np.arange(1, n + 1)
gini = (2 * np.sum(index * sorted_crimes)) / (n * np.sum(sorted_crimes)) - (n + 1) / n
print(f"\n6.4 √çndice de Gini (0=igualdad, 1=m√°xima desigualdad): {gini:.4f}")
if gini > 0.7:
    print(f"   ‚úì MUY CONCENTRADO - Hotspots muy claros y definidos")
elif gini > 0.5:
    print(f"   ‚úì CONCENTRADO - Hotspots moderados")
else:
    print(f"   ‚ö†Ô∏è  POCO concentrado - Distribuci√≥n casi uniforme")

# Persistencia temporal de hotspots
print(f"\n6.5 Persistencia de Hotspots en el Tiempo:")
df_sorted = df.sort_values('fecha_hora_hecho')
mid = len(df_sorted) // 2

df_p1 = df_sorted.iloc[:mid]
df_p2 = df_sorted.iloc[mid:]

cells_p1 = df_p1.groupby('grid_cell').size()
cells_p2 = df_p2.groupby('grid_cell').size()

top_n = 50
top_cells_p1 = set(cells_p1.nlargest(top_n).index)
top_cells_p2 = set(cells_p2.nlargest(top_n).index)
overlap = len(top_cells_p1 & top_cells_p2)

print(f"   Per√≠odo 1: {df_p1['fecha_hora_hecho'].min()} a {df_p1['fecha_hora_hecho'].max()}")
print(f"   Per√≠odo 2: {df_p2['fecha_hora_hecho'].min()} a {df_p2['fecha_hora_hecho'].max()}")
print(f"   Top {top_n} hotspots que se repiten: {overlap} ({overlap/top_n*100:.1f}%)")

if overlap > 35:
    print(f"   ‚úì ALTA PERSISTENCIA - Hotspots estables en el tiempo")
elif overlap > 25:
    print(f"   ‚úì PERSISTENCIA MODERADA")
else:
    print(f"   ‚ö†Ô∏è  BAJA PERSISTENCIA - Hotspots cambian mucho")

# Correlaci√≥n espacial entre per√≠odos
all_cells = list(set(cells_p1.index) | set(cells_p2.index))
counts_p1 = [cells_p1.get(cell, 0) for cell in all_cells]
counts_p2 = [cells_p2.get(cell, 0) for cell in all_cells]
spatial_corr = np.corrcoef(counts_p1, counts_p2)[0, 1]

print(f"\n   Correlaci√≥n espacial entre per√≠odos: {spatial_corr:.4f}")
if spatial_corr > 0.7:
    print(f"   ‚úì MUY PREDECIBLE espacialmente")
elif spatial_corr > 0.5:
    print(f"   ‚úì MODERADAMENTE predecible")
else:
    print(f"   ‚ö†Ô∏è  POCO predecible espacialmente")

# ============================================================================
# 7. EVALUACI√ìN DE BASELINES
# ============================================================================
print("\n" + "="*80)
print("7. COMPARACI√ìN CON MODELOS BASELINE")
print("="*80)

# Cargar features procesadas para comparar
try:
    df_features = pd.read_parquet('data/processed/hotspot_features_robo_agravado_lima.parquet')

    if 'crime_count' in df_features.columns:
        print("\n7.1 Baseline: Predecir la Media")
        y_true = df_features['crime_count'].values
        y_pred_mean = np.full_like(y_true, y_true.mean(), dtype=float)

        mae_mean = np.mean(np.abs(y_true - y_pred_mean))
        rmse_mean = np.sqrt(np.mean((y_true - y_pred_mean)**2))
        r2_mean = r2_score(y_true, y_pred_mean)

        print(f"   MAE:  {mae_mean:.4f}")
        print(f"   RMSE: {rmse_mean:.4f}")
        print(f"   R¬≤:   {r2_mean:.4f}")

        print("\n7.2 Baseline: Persistencia (semana anterior)")
        if 'crime_count_lag_1' in df_features.columns:
            mask = df_features['crime_count_lag_1'].notna() & df_features['crime_count'].notna()
            y_true_p = df_features.loc[mask, 'crime_count'].values
            y_pred_p = df_features.loc[mask, 'crime_count_lag_1'].values

            mae_pers = np.mean(np.abs(y_true_p - y_pred_p))
            rmse_pers = np.sqrt(np.mean((y_true_p - y_pred_p)**2))
            r2_pers = r2_score(y_true_p, y_pred_p)

            print(f"   MAE:  {mae_pers:.4f}")
            print(f"   RMSE: {rmse_pers:.4f}")
            print(f"   R¬≤:   {r2_pers:.4f}")

            print("\n7.3 Comparaci√≥n")
            r2_lstm = 0.6970
            print(f"   {'Modelo':<25} {'R¬≤':>10}")
            print(f"   {'-'*35}")
            print(f"   {'Predecir Media':<25} {r2_mean:>10.4f}")
            print(f"   {'Persistencia (t-1)':<25} {r2_pers:>10.4f}")
            print(f"   {'LSTM Optimizado':<25} {r2_lstm:>10.4f}")

            if r2_lstm > r2_pers:
                mejora = ((r2_lstm - r2_pers) / abs(r2_pers) * 100) if r2_pers != 0 else float('inf')
                print(f"\n   ‚úì LSTM supera persistencia en {mejora:.1f}%")
                print(f"   ‚úì El modelo S√ç aporta valor sobre baselines simples")
            else:
                print(f"\n   ‚ö†Ô∏è  LSTM no mejora significativamente sobre persistencia")

except FileNotFoundError:
    print("\n   ‚ö†Ô∏è  No se encontr√≥ el archivo de features procesadas")
    print("   Ejecuta primero: python -m 02_feature_engineering.2b_create_grid_features_lima")

# ============================================================================
# 8. VISUALIZACIONES
# ============================================================================
print("\n" + "="*80)
print("8. GENERANDO VISUALIZACIONES...")
print("="*80)

fig = plt.figure(figsize=(20, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

# 8.1 Serie temporal
ax1 = fig.add_subplot(gs[0, :])
crimes_per_week.plot(ax=ax1, color='steelblue', linewidth=1.5)
ax1.set_title('Serie Temporal: Cr√≠menes por Semana', fontweight='bold', fontsize=14)
ax1.set_xlabel('Semana')
ax1.set_ylabel('N√∫mero de Cr√≠menes')
ax1.grid(True, alpha=0.3)

# 8.2 Estacionalidad mensual
ax2 = fig.add_subplot(gs[1, 0])
por_mes.plot(kind='bar', ax=ax2, color='coral')
ax2.set_title('Estacionalidad Mensual', fontweight='bold')
ax2.set_xlabel('Mes')
ax2.set_ylabel('Total Cr√≠menes')
ax2.set_xticklabels(meses, rotation=45)
ax2.grid(True, alpha=0.3, axis='y')

# 8.3 Patr√≥n semanal
ax3 = fig.add_subplot(gs[1, 1])
por_dia.plot(kind='bar', ax=ax3, color='mediumseagreen')
ax3.set_title('Patr√≥n D√≠a de Semana', fontweight='bold')
ax3.set_xlabel('D√≠a')
ax3.set_ylabel('Total Cr√≠menes')
ax3.set_xticklabels(['L', 'M', 'X', 'J', 'V', 'S', 'D'], rotation=0)
ax3.grid(True, alpha=0.3, axis='y')

# 8.4 Autocorrelaci√≥n
ax4 = fig.add_subplot(gs[1, 2])
autocorrs = []
for lag in range(1, 13):
    if len(crimes_per_week) > lag:
        shifted = crimes_per_week.shift(lag)
        valid_mask = ~(crimes_per_week.isna() | shifted.isna())
        if valid_mask.sum() > 10:
            corr = crimes_per_week[valid_mask].corr(shifted[valid_mask])
            autocorrs.append(corr)
ax4.bar(range(1, len(autocorrs)+1), autocorrs, color='teal', alpha=0.7)
ax4.axhline(y=0.3, color='red', linestyle='--', linewidth=1, label='Moderado')
ax4.set_title('Autocorrelaci√≥n Temporal', fontweight='bold')
ax4.set_xlabel('Lag (semanas)')
ax4.set_ylabel('Correlaci√≥n')
ax4.legend()
ax4.grid(True, alpha=0.3)

# 8.5 Curva de Pareto
ax5 = fig.add_subplot(gs[2, 0])
sorted_cumsum = (crimes_per_cell.sort_values(ascending=False).cumsum() / crimes_per_cell.sum() * 100)
x_pct = np.arange(1, len(sorted_cumsum) + 1) / len(sorted_cumsum) * 100
ax5.plot(x_pct, sorted_cumsum.values, linewidth=2, color='darkviolet')
ax5.axhline(y=80, color='red', linestyle='--', linewidth=1, label='80%')
ax5.set_title('Curva Pareto - Concentraci√≥n Espacial', fontweight='bold')
ax5.set_xlabel('% Celdas (ordenadas)')
ax5.set_ylabel('% Acumulado Cr√≠menes')
ax5.legend()
ax5.grid(True, alpha=0.3)

# 8.6 Distribuci√≥n cr√≠menes por celda
ax6 = fig.add_subplot(gs[2, 1])
ax6.hist(crimes_per_cell.values, bins=50, color='orange', edgecolor='black', alpha=0.7)
ax6.set_title('Distribuci√≥n Cr√≠menes/Celda', fontweight='bold')
ax6.set_xlabel('Cr√≠menes por Celda')
ax6.set_ylabel('Frecuencia')
ax6.set_yscale('log')
ax6.grid(True, alpha=0.3)

# 8.7 Mapa de calor simple
ax7 = fig.add_subplot(gs[2, 2])
pivot_data = df.groupby(['grid_lat', 'grid_long']).size().reset_index(name='count')
pivot_data = pivot_data.pivot(index='grid_lat', columns='grid_long', values='count').fillna(0)
sns.heatmap(pivot_data, cmap='YlOrRd', cbar_kws={'label': 'Cr√≠menes'}, ax=ax7, robust=True)
ax7.set_title('Mapa de Calor - Hotspots Lima', fontweight='bold')
ax7.set_xlabel('Longitud (grid)')
ax7.set_ylabel('Latitud (grid)')

plt.savefig('validacion_metodologia_completa.png', dpi=300, bbox_inches='tight')
print("‚úì Visualizaci√≥n guardada: 'validacion_metodologia_completa.png'")

# ============================================================================
# 9. CONCLUSIONES
# ============================================================================
print("\n" + "="*80)
print("9. CONCLUSIONES Y RECOMENDACIONES")
print("="*80)

print("\nüìä RESUMEN EJECUTIVO:")
print("-" * 80)

# Criterios de evaluaci√≥n
score = 0
max_score = 5

# 1. Patrones temporales
tiene_estacionalidad = cv_mes > 0.15 or max(autocorrs[:4]) > 0.3
if tiene_estacionalidad:
    score += 1
    print("\n‚úì [1/5] HAY patrones temporales predecibles")
else:
    print("\n‚ö†Ô∏è  [0/5] Patrones temporales d√©biles")

# 2. Hotspots espaciales
hotspots_claros = gini > 0.5
if hotspots_claros:
    score += 1
    print("‚úì [2/5] HAY hotspots espaciales claramente definidos")
else:
    print("‚ö†Ô∏è  [1/5] Hotspots poco definidos")

# 3. Persistencia temporal
persistencia_alta = overlap > 30
if persistencia_alta:
    score += 1
    print("‚úì [3/5] Hotspots SON persistentes en el tiempo")
else:
    print("‚ö†Ô∏è  [2/5] Baja persistencia de hotspots")

# 4. Correlaci√≥n espacial
correlacion_alta = spatial_corr > 0.6
if correlacion_alta:
    score += 1
    print("‚úì [4/5] ALTA correlaci√≥n espacial entre per√≠odos")
else:
    print("‚ö†Ô∏è  [3/5] Correlaci√≥n espacial moderada")

# 5. Modelo supera baselines
if 'r2_pers' in locals():
    supera_baseline = 0.6970 > r2_pers
    if supera_baseline:
        score += 1
        print("‚úì [5/5] Modelo LSTM supera baselines simples")
    else:
        print("‚ö†Ô∏è  [4/5] Modelo no supera significativamente a persistencia")

print(f"\nüéØ SCORE FINAL: {score}/{max_score}")
print("=" * 80)

if score >= 4:
    print("\n‚úÖ CONCLUSI√ìN: S√ç VALE LA PENA hacer predicciones de hotspots")
    print("\nüìã JUSTIFICACI√ìN:")
    print("  ‚Ä¢ Los datos muestran patrones temporales y espaciales claros")
    print("  ‚Ä¢ Los hotspots son persistentes y predecibles")
    print("  ‚Ä¢ El modelo LSTM aporta valor real sobre m√©todos simples")
    print("  ‚Ä¢ R¬≤ de 0.697 significa que el modelo explica ~70% de la variabilidad")
    print("\nüí° RECOMENDACIONES:")
    print("  ‚úì La metodolog√≠a es correcta (divisi√≥n temporal, features de lag)")
    print("  ‚úì El modelo es √∫til para PRIORIZAR zonas de patrullaje")
    print("  ‚úì Enfoque: Predicci√≥n de RIESGO RELATIVO, no conteo exacto")
    print("  ‚úì Valor operacional: Asignaci√≥n proactiva de recursos")
    print("\nüîß MEJORAS SUGERIDAS:")
    print("  ‚Ä¢ Recalibraci√≥n para reducir sobreestimaci√≥n")
    print("  ‚Ä¢ Incorporar variables externas (clima, eventos, d√≠as festivos)")
    print("  ‚Ä¢ An√°lisis de texto en 'observacion_hecho' con NLP")

elif score >= 3:
    print("\n‚ö†Ô∏è  CONCLUSI√ìN: Las predicciones tienen VALOR LIMITADO")
    print("\nüìã JUSTIFICACI√ìN:")
    print("  ‚Ä¢ Se detectan algunos patrones pero no son muy fuertes")
    print("  ‚Ä¢ El modelo aporta cierto valor pero con limitaciones")
    print("\nüí° RECOMENDACIONES:")
    print("  ‚Ä¢ Usar como herramienta complementaria, no principal")
    print("  ‚Ä¢ Combinar con conocimiento experto policial")
    print("  ‚Ä¢ Evaluar costo-beneficio del desarrollo")

else:
    print("\n‚ùå CONCLUSI√ìN: Las predicciones NO parecen valer la pena")
    print("\nüìã JUSTIFICACI√ìN:")
    print("  ‚Ä¢ Patrones d√©biles o inexistentes")
    print("  ‚Ä¢ Hotspots no son suficientemente persistentes")
    print("  ‚Ä¢ El modelo no supera significativamente a baselines")
    print("\nüí° RECOMENDACIONES:")
    print("  ‚Ä¢ Re-evaluar la definici√≥n del problema")
    print("  ‚Ä¢ Considerar otros tipos de delitos")
    print("  ‚Ä¢ Incorporar muchas m√°s variables externas")

print("\n" + "="*80)
print("AN√ÅLISIS COMPLETADO")
print("="*80)
